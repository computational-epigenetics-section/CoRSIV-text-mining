{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Wen-Jou Chang\n",
    "Baylor College of Medicine\n",
    "\n",
    "This notebook performs a systematic search of PubMed articles related to various disease categories.\n",
    "\n",
    "The pipeline:\n",
    "1. Searches PubMed for articles in different disease categories\n",
    "2. Analyzes articles for:\n",
    "    - Full text availability\n",
    "    - Mentions of methylation probes or Illumina-related keywords in main text/tables\n",
    "    - Supplementary data availability\n",
    "    - extract CG probe mentions\n",
    "4. Collects and organizes files by disease category\n",
    "5. Extracts probes from supplementary files then unify all probes into one big file for each category\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialization\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict as dd\n",
    "from http.client import IncompleteRead\n",
    "from pandas.errors import EmptyDataError \n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "\n",
    "os.chdir(\"YOUR DIRECTORY\")\n",
    "\n",
    "obesity = [\"Obesity\"]\n",
    "cancer = [\"Neoplasms\"]\n",
    "cardiovascular = [\"Cardiovascular Diseases\"]\n",
    "digestive = [\"Digestive System Diseases\"]\n",
    "endocrine = [\"Endocrine System Diseases\"]\n",
    "hematological = [\"Hemic and Lymphatic Diseases\"]\n",
    "immune = [\"Immune System Diseases\"]\n",
    "metabolic = [\"Metabolic Diseases\"]\n",
    "neurological = [\"Mental Disorders\", \"Nervous System Diseases\"]\n",
    "urogenital = [\"Urogenital Diseases\"]\n",
    "respiratory = [\"Respiratory Tract Diseases\"]\n",
    "\n",
    "categories = [obesity, cancer, cardiovascular, digestive, endocrine, hematological, immune, metabolic, neurological, urogenital, respiratory]\n",
    "category_names = [\"obesity\", \"cancer\", \"cardiovascular\", \"digestive\", \"endocrine\", \"hematological\", \"immune\", \"metabolic\", \"neurological\", \"urogenital\", \"respiratory\"]\n",
    "C = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get articles from pubmed in each disease category.\n",
    "We performed our search on January 16th, 2024; studies published thereafter are not included in our analysis.\n",
    "\"\"\"\n",
    "def search(query):\n",
    "    Entrez.email = 'u239646@bcm.edu'\n",
    "    handle = Entrez.esearch(db='pubmed',\n",
    "                            sort='relevance',\n",
    "                            retmax='10000',\n",
    "                            retmode='xml',\n",
    "                            term=query,)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "def fetch_details(id_list):\n",
    "    ids = ','.join(id_list)\n",
    "    Entrez.email = 'u239646@bcm.edu'\n",
    "    try:\n",
    "        handle = Entrez.efetch(db='pubmed',\n",
    "                            retmode='xml',\n",
    "                            id=ids)\n",
    "        results = Entrez.read(handle, validate=False)\n",
    "        handle.close()\n",
    "    except IncompleteRead:\n",
    "        handle = Entrez.efetch(db='pubmed',\n",
    "                            retmode='xml',\n",
    "                            id=ids)\n",
    "        results = Entrez.read(handle, validate=False)\n",
    "        handle.close()\n",
    "    return results\n",
    "\n",
    "def safe_get(data, keys, default=\"\"):\n",
    "    try:\n",
    "        for key in keys:\n",
    "            data = data[key]\n",
    "        return data\n",
    "    except (KeyError, TypeError):\n",
    "        return default\n",
    "\n",
    "def process_mesh_terms(mesh_list):\n",
    "    mesh_list_processed = []\n",
    "    for mesh_term in mesh_list:\n",
    "        mesh = mesh_term[\"DescriptorName\"]\n",
    "        qualifier = mesh_term[\"QualifierName\"]\n",
    "        if len(qualifier) > 0:\n",
    "            for q in qualifier:\n",
    "                mesh_list_processed.append(mesh+\"/\"+q)\n",
    "        else:\n",
    "            mesh_list_processed.append(mesh)\n",
    "    paper_mesh = \" | \".join(mesh_list_processed)\n",
    "    return paper_mesh\n",
    "\n",
    "def pad_keywords(kw_list):\n",
    "    if len(kw_list) == 1:\n",
    "        return f'\"{kw_list[0]}\"[MeSH Terms]'\n",
    "    initial = '(\"'\n",
    "    kw = '\"[MeSH Terms] OR \"'.join(kw_list)\n",
    "    initial += kw\n",
    "    initial += '\"[MeSH Terms])'\n",
    "    return initial\n",
    "\n",
    "def full_search_term(kw):\n",
    "    return f'(\"DNA Methylation\"[MeSH Terms] AND {kw}) NOT (\"animals\"[MeSH Terms] NOT \"humans\"[MeSH Terms]) AND \"pubmed pmc\"[Filter]'\n",
    "\n",
    "def get_papers(query, output_name, file_name=None):\n",
    "    \n",
    "    pmids = []\n",
    "    titles = []\n",
    "    last_name = []\n",
    "    journal = []\n",
    "    publication_types = []\n",
    "    journal_iso = []\n",
    "    year = []\n",
    "    abstract = []\n",
    "    mesh_terms = []\n",
    "    pmcid = []\n",
    "\n",
    "    if file_name:\n",
    "        with open(file_name, 'r') as file:\n",
    "            id_list = [line.strip() for line in file]\n",
    "    else:\n",
    "        query = full_search_term(pad_keywords(query))\n",
    "        results = search(query)\n",
    "        id_list = results['IdList']\n",
    "    \n",
    "    count = len(id_list)\n",
    "\n",
    "    if count > 0 and count < 10000:\n",
    "        papers = fetch_details(id_list)\n",
    "        for paper in papers['PubmedArticle']:\n",
    "            pmids.append(safe_get(paper, ['MedlineCitation', 'PMID']))\n",
    "            titles.append(safe_get(paper, ['MedlineCitation', 'Article', 'ArticleTitle']))\n",
    "            last_name.append(safe_get(paper, ['MedlineCitation', 'Article', 'AuthorList', 0, 'LastName']))\n",
    "            journal.append(safe_get(paper, ['MedlineCitation', 'Article', 'Journal', 'Title']))\n",
    "            journal_iso.append(safe_get(paper, ['MedlineCitation', 'Article', 'Journal', 'ISOAbbreviation']))\n",
    "            year.append(safe_get(paper, ['MedlineCitation', 'Article', 'Journal', 'JournalIssue', 'PubDate', 'Year']))\n",
    "            abstract.append(\" \".join(map(str, safe_get(paper, ['MedlineCitation', 'Article', 'Abstract', 'AbstractText'], []))))\n",
    "            mesh_terms.append(process_mesh_terms(safe_get(paper, ['MedlineCitation', 'MeshHeadingList'], [])))\n",
    "            ids = safe_get(paper, ['PubmedData', 'ArticleIdList'], [])\n",
    "            # print(ids)\n",
    "            pmcid.append(next((i for i in ids if i.startswith(\"PMC\")), np.nan))\n",
    "    elif count >= 10000:\n",
    "        for i in range(0, count, 5000):\n",
    "            print(i, min(i+5000, count))\n",
    "            papers = fetch_details(id_list[i:min(i+5000, count)])\n",
    "            for paper in papers['PubmedArticle']:\n",
    "                pmids.append(safe_get(paper, ['MedlineCitation', 'PMID']))\n",
    "                titles.append(safe_get(paper, ['MedlineCitation', 'Article', 'ArticleTitle']))\n",
    "                last_name.append(safe_get(paper, ['MedlineCitation', 'Article', 'AuthorList', 0, 'LastName']))\n",
    "                journal.append(safe_get(paper, ['MedlineCitation', 'Article', 'Journal', 'Title']))\n",
    "                publication_types.append(\";\".join(set([str(i) for i in safe_get(paper, ['MedlineCitation', 'Article', 'PublicationTypeList']) if not str(i).startswith(\"Research Support\")])))\n",
    "                journal_iso.append(safe_get(paper, ['MedlineCitation', 'Article', 'Journal', 'ISOAbbreviation']))\n",
    "                year.append(safe_get(paper, ['MedlineCitation', 'Article', 'Journal', 'JournalIssue', 'PubDate', 'Year']))\n",
    "                abstract.append(\" \".join(map(str, safe_get(paper, ['MedlineCitation', 'Article', 'Abstract', 'AbstractText'], []))))\n",
    "                mesh_terms.append(process_mesh_terms(safe_get(paper, ['MedlineCitation', 'MeshHeadingList'], [])))\n",
    "                ids = safe_get(paper, ['PubmedData', 'ArticleIdList'], [])\n",
    "                # print(ids)\n",
    "                pmcid.append(next((i for i in ids if i.startswith(\"PMC\")), np.nan))\n",
    "\n",
    "    df = pd.DataFrame({\"Mesh Term\": mesh_terms, \"PMID\": pmids, \"PMCID\": pmcid, \"Last Name\": last_name, \"Year\": year, \"Journal\": journal, \"ISO_journal\": journal_iso, \"Title\": titles, \"Abstract\": abstract, \"Publication Type\": publication_types})\n",
    "    df.to_csv(output_name, index=0)\n",
    "\n",
    "for i in range(C):\n",
    "    # esearch -db pubmed -query \"[query term]\" | efetch -format uid > output.txt for records >= 10000\n",
    "    if i == 7 or i == 1:\n",
    "        get_papers(categories[i], f\"pubmed_search/{category_names[i]}.csv\", f\"pubmed_search/{category_names[i]}.txt\")\n",
    "    else:\n",
    "        get_papers(categories[i], f\"pubmed_search/{category_names[i]}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get full texts if available. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def fetch_full_text(session, pmid):\n",
    "    url = f'https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_json/{pmid}/ascii'\n",
    "    async with session.get(url) as response:\n",
    "        data = await response.text()\n",
    "        if data.startswith(\"[Error]\"):\n",
    "            return \"\"\n",
    "        return data\n",
    "\n",
    "async def process_probes(pmids):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_full_text(session, pmid) for pmid in pmids]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "for i in range(C):\n",
    "    df = pd.read_csv(f\"pubmed_search/{category_names[i]}.csv\")\n",
    "    pmids = df[\"PMID\"].tolist()\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    passages = loop.run_until_complete(process_probes(pmids))\n",
    "\n",
    "\n",
    "    df2 = pd.DataFrame({\"PMID\": pmids, \"PMCID\": df[\"PMCID\"], \"Full Text\": passages})\n",
    "    df2.to_csv(f\"full_text/{category_names[i]}_full_text.csv\", index=False)\n",
    "    print(category_names[i]+\" done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get probes from only table in main text.\n",
    "\"\"\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "CAPTION = 1\n",
    "TABLE = 2\n",
    "FOOTNOTE = 3\n",
    "TITLE_CAPTION = 4\n",
    "table_key = {'table_caption':CAPTION,\n",
    "'table':TABLE,\n",
    "'table_footnote':FOOTNOTE,\n",
    "'table_title_caption':TITLE_CAPTION}\n",
    "\n",
    "def fetch_probe_from_table(pmid, text, pmcid):\n",
    "    probe_result_set = set()\n",
    "    if type(text) is float:\n",
    "        return\n",
    "    paragraphs = json.loads(text)[\"documents\"][0][\"passages\"]\n",
    "    table_dict = dd(lambda: dd(list))\n",
    "    for p_idx in range(len(paragraphs)):\n",
    "        p = paragraphs[p_idx]\n",
    "        if p[\"infons\"][\"section_type\"] == \"TABLE\":\n",
    "            try:\n",
    "                table_id = p[\"infons\"]['id']\n",
    "            except:\n",
    "                # print(p[\"infons\"])\n",
    "                continue\n",
    "            part = p[\"infons\"][\"type\"]\n",
    "            if part == \"table\":\n",
    "                table_dict[table_id][table_key.get(part, -1)].append((p[\"infons\"][\"xml\"]))\n",
    "            else:\n",
    "                table_dict[table_id][table_key.get(part, -1)].append((p[\"text\"].lower()))\n",
    "    probe_pattern = r'cg\\d{8}'\n",
    "    for table_id, table_content in table_dict.items():\n",
    "        has_table = table_content.get(TABLE, None)\n",
    "        if has_table:\n",
    "            has_table = [t for t in has_table if t]\n",
    "            curr_table = \" \".join(has_table)\n",
    "            probe = re.findall(probe_pattern, curr_table)\n",
    "            probe_result_set.update(set(probe))\n",
    "        # else:\n",
    "            # print(pmid)\n",
    "\n",
    "    for p in probe_result_set:\n",
    "        probe_id.append(p)\n",
    "        probe_pmid.append(pmid)\n",
    "        probe_pmcid.append(pmcid)\n",
    "        \n",
    "for cat in category_names:\n",
    "    data = pd.read_csv(f\"full_text/{cat}_full_text.csv\")\n",
    "    probe_id = []\n",
    "    probe_pmid = []\n",
    "    probe_pmcid = []\n",
    "\n",
    "    for j, row in data.iterrows():\n",
    "        # print(row[\"PMID\"])\n",
    "        fetch_probe_from_table(row[\"PMID\"], row[\"Full Text\"], row[\"PMCID\"])\n",
    "\n",
    "    df2 = pd.DataFrame({\"probeId\": probe_id, \"pmid\": probe_pmid, \"pmcid\": probe_pmcid})\n",
    "    df2.to_csv(f\"probe_main_table/{cat}_main_probes.csv\", index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get probes from main text.\n",
    "\"\"\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def fetch_probe_from_text(pmid, text, pmcid):\n",
    "    probe_result_set = set()\n",
    "    if type(text) is float:\n",
    "        return\n",
    "    paragraphs = [p.get(\"text\") for p in json.loads(text)[\"documents\"][0][\"passages\"] if p.get(\"text\")]\n",
    "    # print(json.dumps(paragraphs, indent=4))\n",
    "\n",
    "    # print(type(paragraphs))\n",
    "    probe_pattern = r'cg\\d{8}'\n",
    "    curr_table = \" \".join(paragraphs)\n",
    "    probe = re.findall(probe_pattern, curr_table)\n",
    "    probe_result_set.update(set(probe))\n",
    "\n",
    "    for p in probe_result_set:\n",
    "        probe_id.append(p)\n",
    "        probe_pmid.append(pmid)\n",
    "        probe_pmcid.append(pmcid)\n",
    "        \n",
    "for cat in category_names:\n",
    "    data = pd.read_csv(f\"full_text/{cat}_full_text.csv\")\n",
    "    probe_id = []\n",
    "    probe_pmid = []\n",
    "    probe_pmcid = []\n",
    "\n",
    "    for j, row in data.iterrows():\n",
    "        fetch_probe_from_text(row[\"PMID\"], row[\"Full Text\"], row[\"PMCID\"])\n",
    "\n",
    "    df2 = pd.DataFrame({\"probeId\": probe_id, \"pmid\": probe_pmid, \"pmcid\": probe_pmcid})\n",
    "    df2.to_csv(f\"probe_main_text/{cat}_main_probes.csv\", index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Identify papers mentioning Illumina-related keywords: \"450K\", \"HM450\", \"EPIC\", \"450k\", \"HumanMethylation450\", \"850K\", \"850k\".\n",
    "\"\"\"\n",
    "download_supplementary_subset = []\n",
    "def contains_keywords(input_string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in input_string:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for cat in category_names:\n",
    "    kw_illumina = []\n",
    "    df = pd.read_csv(f\"full_text/{cat}_full_text.csv\")\n",
    "    for j, row in df.iterrows():\n",
    "        keywords = [\"450K\", \"HM450\", \"EPIC\", \"450k\", \"HumanMethylation450\", \"850K\", \"850k\"]\n",
    "        text = row[\"Full Text\"]\n",
    "        if type(text) is float:\n",
    "            continue\n",
    "        paragraphs = [p.get(\"text\") for p in json.loads(text)[\"documents\"][0][\"passages\"] if p.get(\"text\") and p[\"infons\"][\"section_type\"]!=\"REF\"]\n",
    "        text = \" \".join(paragraphs)\n",
    "        is_illumina_study = contains_keywords(text, keywords)\n",
    "        if is_illumina_study:\n",
    "            kw_illumina.append(row[\"PMCID\"])\n",
    "\n",
    "    kw_illumina = set(kw_illumina)\n",
    "    df = pd.read_csv(f\"probe_main_text/{cat}_main_probes.csv\")\n",
    "    mention_cg_id = set(df[\"pmcid\"].to_list())\n",
    "    download_supplementary_subset.append(mention_cg_id.union(kw_illumina))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get supplementary file download links for targeted papers from PMCOA.\n",
    "\"\"\"\n",
    "\n",
    "async def fetch_link(session, pmcid):\n",
    "    url = f\"https://ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id={pmcid}\"\n",
    "    async with session.get(url) as response:\n",
    "        data = await response.text()\n",
    "        href_pattern = re.compile(r'<link[^>]*href=\"([^\"]+)\"')\n",
    "        match = href_pattern.search(data)\n",
    "        href_value = match.group(1) if match else None\n",
    "        return href_value\n",
    "\n",
    "\n",
    "async def process_link(pmids):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_link(session, pmid) for pmid in pmids]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "for i in range(4, len(category_names)):\n",
    "    cat = category_names[i]\n",
    "    pmcids = list(download_supplementary_subset[i])\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    passages = loop.run_until_complete(process_link(pmcids))\n",
    "\n",
    "    df2 = pd.DataFrame({\"PMCID\": pmcids, \"Download Link\": passages})\n",
    "    df2.to_csv(f\"download_links/{cat}_link.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# codes in this section were run in the cluster as slurm files, cat is the category name\n",
    "\n",
    "# download all supplementary files based on link \n",
    "wget -i ${cat}_link.txt -c\n",
    "\n",
    "# unzip\n",
    "for file in *.tar.gz; do\n",
    "   tar -xvzf \"$file\" -C . && mv \"$file\" ../tar_archive\n",
    "   subfolder_name=$(basename \"$file\" .tar.gz)\n",
    "   for file1 in \"$subfolder_name\"/*; do\n",
    "       mv \"$file1\" \"${file1/\\//-}\"\n",
    "   done\n",
    "rm -d $subfolder_name\n",
    "done\n",
    "\n",
    "# reorganize files based on file extension\n",
    "exts=$(ls | sed 's/^.*\\.//' | sort -u)\n",
    "for ext in $exts\n",
    "do\n",
    "    echo Processing \"$ext\"\n",
    "    mkdir -p \"$ext\"\n",
    "    mv -vn *.\"$ext\" \"$ext\"/\n",
    "done\n",
    "\n",
    "# unzip nested zipped files: first run handle_zip.py then delete empty folders\n",
    "python3 handle_zip.py \"${cat}\"\n",
    "find . -empty -type d -delete\n",
    "\n",
    "# optional: convert Excel files (.xls/.xlsx) to CSV format. While this adds some processing overhead initially,\n",
    "# it significantly speeds up subsequent file access and analysis\n",
    "while IFS= read -r file; do\n",
    "    while IFS= read -r sheet; do\n",
    "        in2csv --sheet \"$sheet\" \"$file\" > \"${file%.*}-${sheet}.csv\" # need to install in2csv first\n",
    "    done < <(in2csv -n \"$file\")\n",
    "done < <(find . -name '*.xls' -o -name '*.xlsx')\n",
    "\n",
    "# extract probes from supplementary files\n",
    "python3 get_probe_supplementary.py \"${cat}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Combine probes scraped from different file extensions into one for each category.\n",
    "\"\"\"\n",
    "\n",
    "cat_probes_dict = []\n",
    "def combine_probes(input_cat):\n",
    "    initial_df = pd.read_csv(f\"probe_main_text/{input_cat}_main_probes.csv\")\n",
    "    initial_df = initial_df[[\"pmcid\", \"probeId\"]]\n",
    "    dfs = [initial_df]\n",
    "    for type in [\"csv\", \"xls\", \"xlsx\", \"xlsb\", \"xlsm\", \"failed\"]:\n",
    "        if os.path.exists(f\"probe/{input_cat.split('_')[0]}_{type}_probes.csv\"):\n",
    "            try:\n",
    "                temp = pd.read_csv(f\"probe/{input_cat.split('_')[0]}_{type}_probes.csv\")\n",
    "                dfs.append(temp)\n",
    "            except EmptyDataError:\n",
    "                continue\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "    result_df.drop_duplicates(inplace=True)\n",
    "    pubmed_paper = pd.read_csv(f\"pubmed_search/{input_cat}.csv\")\n",
    "    \n",
    "    # skip non-research papers\n",
    "    pubmed_paper[\"Publication Type\"] = pubmed_paper[\"Publication Type\"].apply(lambda x: [y.strip() for y in x.split(\";\")])\n",
    "    skip = [\"Meta-Analysis\", \"Review\", \"Systematic Review\", \"Retracted Publication\", \"Comment\", \"Video-Audio Media\", \"Validation Study\", \"Historical Article\"]\n",
    "    pubmed_paper = pubmed_paper[pubmed_paper[\"Publication Type\"].apply(lambda x: \"Journal Article\" in x and not any(skip_type in x for skip_type in skip))]\n",
    "    \n",
    "    pubmed_paper = pubmed_paper[[\"Category Mesh Term\", \"PMCID\"]]\n",
    "    pubmed_paper.rename(columns={\"Category Mesh Term\": \"Filtered Mesh Term\", \"PMCID\": \"pmcid\"}, inplace=True)\n",
    "    output = pd.merge(pubmed_paper, result_df, how=\"inner\", on=\"pmcid\")\n",
    "    output.to_csv(f\"probe/{input_cat}_all_probes.csv\", index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
